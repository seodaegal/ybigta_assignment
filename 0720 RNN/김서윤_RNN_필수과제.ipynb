{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Recurrent Neural Networks - 필수 과제**\n",
    "\n",
    "**LSTM**을 구현해봅시다!\n",
    "<br><br><br>\n",
    "**필요 사전 지식**:\n",
    "\n",
    "- <u>PyTorch</u> (선택 과제 1)\n",
    "\n",
    "<br>\n",
    "\n",
    "**추가 사전 지식**: (알면 좋으나 몰라도 괜찮음)\n",
    "\n",
    "- <u>Tokenization</u>, <u>Word Embedding</u> (선택 과제 2)\n",
    "\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from transformers) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: datasets in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (2.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from datasets) (1.25.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from datasets) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/seodaegal/anaconda3/envs/ybenv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "[Hugging Face](https://huggingface.co)에서 [Rotten Tomatoes dataset](https://huggingface.co/datasets/rotten_tomatoes)과 [pretrained BERT](https://huggingface.co/bert-base-uncased)의 tokenizer를 가져오겠습니다.\n",
    "\n",
    "또 학습 부담을 줄이기 위해 pretrained BERT에 내장된 word embedding layer의 weight도 가져옵시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/rotten_tomatoes\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "# https://huggingface.co/bert-base-uncased\n",
    "pretrained_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "pretrained_embeddings = AutoModel.from_pretrained(\"bert-base-uncased\").embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "기본 BERT는 token을 768차원 벡터로 embedding합니다. 우리의 작은 dataset과 작은 모델에게 768차원은 부담스러우니 PCA를 사용해 64차원으로 줄여줍시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nano_embed = torch.pca_lowrank(pretrained_embeddings.weight.detach(), q=64)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "그런데 무작정 64차원으로 줄여도 되는 걸까요? BERT의 d_model이 괜히 768도 아닐 테고, 정보의 손실이 아주 클 것 같은데 말입니다.\n",
    "\n",
    "궁금하니 코사인 유사도로 축소된 embedding layer에 token들의 정보가 그럭저럭 잘 남아있는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = (nano_embed @ nano_embed.T) / (nano_embed.abs() @ nano_embed.abs().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['him',\n",
       " 'that',\n",
       " 'me',\n",
       " 'i',\n",
       " 'they',\n",
       " 'what',\n",
       " 'this',\n",
       " 'her',\n",
       " 'he',\n",
       " 'who',\n",
       " 'the',\n",
       " ',',\n",
       " '\"',\n",
       " 'and',\n",
       " 'all',\n",
       " 'them',\n",
       " 'it',\n",
       " 'then',\n",
       " 'of',\n",
       " 'is']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word에 다양한 값을 넣어보세요! tokenizer의 vocab에 없는 token에 대해서는 빈 list가 뜹니다.\n",
    "word = \"you\"\n",
    "\n",
    "([*map(pretrained_tokenizer.decode, cos[pretrained_tokenizer.vocab[word]].argsort(descending=True)[1:21])] if word in pretrained_tokenizer.vocab else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "꽤 잘 남아있는 것 같습니다.\n",
    "\n",
    "(TMI: 조금 더 욕심을 부려 한번 32차원으로 줄여보면 무시하기 어려운 정보의 손실을 체감할 수 있습니다.)\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 LSTM을 구현합시다! 사실 원래 BiLSTM으로 하려고 했는데 underfitting이 심해서 그냥 plain LSTM으로 준비했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "#### <span style=\"color:red\">**<u>Q1.</u>**</span>\n",
    "\n",
    "`class LSTMCell`의 빈칸을 채우세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, d_x, d_h): # d_x: x의 차원수 (scalar int)\n",
    "                                  # d_h: h의 차원수 (scalar int)\n",
    "        super().__init__()\n",
    "        d_stack = d_x + d_h\n",
    "        ######################### START OF YOUR CODE #########################\n",
    "\n",
    "        dim1 = d_stack\n",
    "        dim2 = d_h\n",
    "        dim3 = d_stack\n",
    "        dim4 = d_h\n",
    "        dim5 = d_stack\n",
    "        dim6 = d_h\n",
    "\n",
    "        ########################## END OF YOUR CODE ##########################\n",
    "        self.W_f = nn.Linear(d_stack, d_h)\n",
    "        self.W_i = nn.Linear(dim1, dim2)\n",
    "        self.W_C = nn.Linear(dim3, dim4)\n",
    "        self.W_o = nn.Linear(dim5, dim6)\n",
    "\n",
    "\n",
    "    # forward는 t-1의 h_{t-1}, C_{t-1}과 t의 x_t를 입력으로 받아 계산합니다.\n",
    "\n",
    "    def forward(self, x, h, C): # x: x_t\n",
    "                                # h: h_{h-1}\n",
    "                                # C: C_{t-1}\n",
    "        stack = torch.cat([x, h])\n",
    "        ######################### START OF YOUR CODE #########################\n",
    "\n",
    "        f =   self.W_f(stack).sigmoid()\n",
    "        i =   self.W_i(stack).sigmoid()\n",
    "        C_ =  self.W_C(stack).tanh()\n",
    "\n",
    "        C_t = f * C + i * C_\n",
    "\n",
    "        o =   self.W_o(stack).sigmoid()\n",
    "        h_t = o * C_t.tanh()\n",
    "\n",
    "        ########################## END OF YOUR CODE ##########################\n",
    "        return h_t, C_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_out, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        vocab_size = pretrained_embeddings.shape[0]\n",
    "        d_h = d_model = pretrained_embeddings.shape[1]\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, d_model, _weight=pretrained_embeddings.clone()) # word embedding layer\n",
    "        self.cell = LSTMCell(d_x=d_model, d_h=d_h) # LSTM cell\n",
    "        self.out = nn.Linear(d_h, d_out, bias=True) # output layer\n",
    "\n",
    "        self.h_init = nn.Parameter(torch.zeros(d_h), requires_grad=False) # initial h\n",
    "        self.C_init = nn.Parameter(torch.zeros(d_h), requires_grad=False) # initial C\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embed(input_ids).squeeze()\n",
    "\n",
    "        h = self.h_init.clone() # h 초기화\n",
    "        C = self.C_init.clone() # C 초기화\n",
    "        for x in embedded:\n",
    "            h, C = self.cell(x, h, C) # iterate over embedded sequence\n",
    "        \n",
    "        return self.out(h).squeeze() # last hidden state를 output layer에 통과시킨 값을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "#### <span style=\"color:red\">**<u>Q2.</u>**</span>\n",
    "\n",
    "Test accuracy가 0.7 이상이 되도록 모델을 훈련시키세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### START OF YOUR CODE #########################\n",
    "\n",
    "# 필요에 따라 바꿔도 됩니다.\n",
    "device = \"cpu\"\n",
    "\n",
    "########################## END OF YOUR CODE ##########################\n",
    "\n",
    "model = LSTM(vocab_size=pretrained_tokenizer.vocab_size, d_out=1, pretrained_embeddings=nano_embed).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### START OF YOUR CODE #########################\n",
    "\n",
    "# learning rate을 적절히 수정해보세요.\n",
    "lr = 0.0001\n",
    "\n",
    "########################## END OF YOUR CODE ##########################\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset[\"train\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1524/8530 [00:09<00:41, 170.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1500 iter: 0.24808575129508972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 3031/8530 [00:18<00:31, 173.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3000 iter: 0.2754371479358524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 4523/8530 [00:26<00:22, 175.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4500 iter: 0.2657159443969528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 6032/8530 [00:35<00:13, 180.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6000 iter: 0.25081822388495006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7522/8530 [00:44<00:05, 170.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7500 iter: 0.2711720581948757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8530/8530 [00:50<00:00, 169.62it/s]\n"
     ]
    }
   ],
   "source": [
    "######################### START OF YOUR CODE #########################\n",
    "\n",
    "# 필요에 따라 바꿔도 됩니다.\n",
    "num_print = 1500\n",
    "num_batch = 30\n",
    "\n",
    "########################## END OF YOUR CODE ##########################\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "save_l = 0\n",
    "optimizer.zero_grad()\n",
    "for i, data in enumerate(tqdm(train_loader)):\n",
    "    text, label = data[\"text\"][0], data[\"label\"][0]\n",
    "    input_ids = pretrained_tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    y_pred = model(input_ids)\n",
    "\n",
    "    label = label.to(device) * 1.\n",
    "    loss = criterion(y_pred, label)\n",
    "    loss.backward()\n",
    "    \n",
    "    if not (i+1)%num_batch:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    save_l += loss.item()\n",
    "    if not (i+1)%num_print:\n",
    "        print(f\"{i+1:>5} iter: {save_l/num_print}\")\n",
    "        save_l = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1066/1066 [00:01<00:00, 842.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.775797373358349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(dataset[\"test\"], shuffle=True)\n",
    "\n",
    "\n",
    "# test\n",
    "\n",
    "res = torch.tensor(0)\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(test_loader)):\n",
    "        text, label = data[\"text\"][0], data[\"label\"][0]\n",
    "        input_ids = pretrained_tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "        y_pred = model(input_ids)\n",
    "        res += ((1 if y_pred > 0 else 0) == label)\n",
    "\n",
    "print(\"Test accuracy:\", res.item() / dataset[\"test\"].num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'return to never land is much more p . c . than the original version ( no more racist portraits of indians , for instance ) , but the excitement is missing .', 'label': 0}\n",
      "0.1900947242975235\n"
     ]
    }
   ],
   "source": [
    "# 관찰용\n",
    "# n 값을 바꿔가며 훈련시킨 모델의 예측값을 구경해보세요\n",
    "n = 589\n",
    "\n",
    "print(dataset[\"test\"][n])\n",
    "with torch.no_grad():\n",
    "    print(model(pretrained_tokenizer.encode(dataset[\"test\"][n][\"text\"], return_tensors=\"pt\").to(device)).sigmoid().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
